{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b6c793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import multivariate_normal\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def generate_random_samples(n, scale=1.0, size=1000) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate random samples from a multivariate normal distribution.\n",
    "    \n",
    "    Parameters:\n",
    "    n : int - dimension of the distribution\n",
    "    scale : float - scaling factor for the variance magnitudes\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray - samples drawn from the distribution\n",
    "    \"\"\"\n",
    "    mu = [np.random.randn() for _ in range(n)] # does this need to be zero mean?\n",
    "    # Generate a random matrix\n",
    "    A = np.random.randn(n, n) * scale\n",
    "    # Make it positive semi-definite by multiplying with its transpose\n",
    "    cov = A @ A.T\n",
    "    samples = multivariate_normal(mu, cov, size=size)\n",
    "    return samples, mu, cov\n",
    "\n",
    "# Example usage\n",
    "n = 6\n",
    "samples, mu, cov = generate_random_samples(n, scale=2.0, size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2ca58aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original samples (first 5):\n",
      "[[ -2.62090456  14.78740943   0.48413253   4.89910089  -2.43304291\n",
      "    2.33138888]\n",
      " [  2.47418582   0.45865875  -0.92419915   3.97267749   1.65871741\n",
      "   -1.25402435]\n",
      " [ -0.59404646  -3.14665703   2.85125504  -1.52828653  -1.37209772\n",
      "   -0.39330742]\n",
      " [ -3.03238588   7.42815195  -9.82997429  -0.17118683   8.01226121\n",
      "   12.9472815 ]\n",
      " [ -2.02764326  -4.16599263 -11.52986561  -2.62243323   6.73135044\n",
      "    6.40364022]]\n",
      "\n",
      "Discretized samples (first 5):\n",
      "[[1364 4942 3190 4693  999 3523]\n",
      " [4050 2677 2689 4458 2599 2041]\n",
      " [2490 1617 3932 1536 1358 2418]\n",
      " [1166 4353  303 2358 4624 4988]\n",
      " [1675 1331  155  992 4385 4607]]\n",
      "\n",
      "Value range: 1 to 5000\n"
     ]
    }
   ],
   "source": [
    "#nonparametric discretization\n",
    "\n",
    "def discretize_to_percentiles(samples, n_bins=10):\n",
    "    \"\"\"\n",
    "    Discretize each variable in samples into n bins based on percentiles.\n",
    "    \n",
    "    Parameters:\n",
    "    samples : numpy.ndarray - array of shape (n_samples, n_variables)\n",
    "    n_bins : int - number of bins (percentiles) to use\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray - discretized samples with values from 1 to n_bins\n",
    "    \"\"\"\n",
    "    n_samples, n_vars = samples.shape\n",
    "    discretized = np.zeros_like(samples, dtype=int)\n",
    "    \n",
    "    for i in range(n_vars):\n",
    "        # Compute percentile edges for this variable\n",
    "        percentiles = np.linspace(0, 100, n_bins + 1)\n",
    "        bin_edges = np.percentile(samples[:, i], percentiles)\n",
    "        \n",
    "        # Digitize: assign each sample to a bin (1 to n_bins)\n",
    "        discretized[:, i] = np.digitize(samples[:, i], bin_edges[1:-1]) + 1\n",
    "        \n",
    "        # Handle edge case: values exactly at max should be in last bin\n",
    "        discretized[:, i] = np.clip(discretized[:, i], 1, n_bins)\n",
    "    \n",
    "    return discretized\n",
    "\n",
    "# Example usage\n",
    "n_bins = 5000\n",
    "discretized_samples = discretize_to_percentiles(samples, n_bins=n_bins)\n",
    "print(\"Original samples (first 5):\")\n",
    "print(samples[:5])\n",
    "print(\"\\nDiscretized samples (first 5):\")\n",
    "print(discretized_samples[:5])\n",
    "print(\"\\nValue range:\", discretized_samples.min(), \"to\", discretized_samples.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f2e6b5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identification_experiment(samples_train, samples_test, bins):\n",
    "    # For each category of the first variable, learn the mean and covariance of the remaining variables\n",
    "    d = samples_train.shape[1]\n",
    "    cond_stats = {}\n",
    "    for cat in range(1, bins+1):\n",
    "        mask = samples_train[:,0] == cat\n",
    "        # this will produce nan if mask.sum() is too small\n",
    "        if mask.sum() < 2:  # Need at least 2 for covariance\n",
    "            # Use overall statistics as fallback\n",
    "            cond_mean = samples_train[:,1:].mean(axis=0)\n",
    "            cond_cov = np.cov(samples_train[:,1:].T) + np.eye(d-1)*1e-6\n",
    "        else:\n",
    "            cond_mean = samples_train[mask][:,1:].mean(axis=0)\n",
    "            cond_cov = np.cov(samples_train[mask][:,1:].T) + np.eye(d-1)*1e-6 # add small value to diagonal for numerical stability\n",
    "        cond_stats[cat] = (cond_mean, cond_cov)\n",
    "    \n",
    "    # trials = 80% of test samples or max 2000\n",
    "    trials = min(int(samples_test.shape[0] * 0.8), 2000)\n",
    "    ids = np.random.choice(samples_test.shape[0], size=trials, replace=False)\n",
    "    top1_success, avg_rank = 0, 0.0\n",
    "\n",
    "    for tidx in ids:\n",
    "        # observe samples public traits\n",
    "        true_cat = samples_test[tidx,0]\n",
    "        # gather learned stats\n",
    "        mean, cov = cond_stats[true_cat]\n",
    "        # compute log-probabilities for all candidates\n",
    "        candidates = samples_test[:,1:]\n",
    "        logps = stats.multivariate_normal.logpdf(candidates, mean=mean, cov=cov)\n",
    "        ranks = (-logps).argsort().argsort() + 1\n",
    "        true_rank = ranks[tidx]\n",
    "        avg_rank += true_rank\n",
    "        if true_rank == 1:\n",
    "            top1_success += 1\n",
    "\n",
    "    avg_rank /= trials\n",
    "    print(f\"Top-1 Success: {top1_success / trials * 100:.2f}%\")\n",
    "    print(f\"Average Rank: {avg_rank:.2f}\")\n",
    "\n",
    "dsample_train, dsample_test = train_test_split(discretized_samples, test_size=0.2, random_state=42)\n",
    "#identification_experiment(dsample_train, dsample_test, bins=n_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e28e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Identification Experiment with 5 bins ===\n",
      "Top-1 Success: 0.05%\n",
      "Average Rank: 18906.61\n",
      "\n",
      "=== Identification Experiment with 15 bins ===\n",
      "Top-1 Success: 0.00%\n",
      "Average Rank: 18369.84\n",
      "\n",
      "=== Identification Experiment with 25 bins ===\n",
      "Top-1 Success: 0.00%\n",
      "Average Rank: 18202.49\n",
      "\n",
      "=== Identification Experiment with 35 bins ===\n",
      "Top-1 Success: 0.00%\n",
      "Average Rank: 17919.06\n",
      "\n",
      "=== Identification Experiment with 45 bins ===\n",
      "Top-1 Success: 0.00%\n",
      "Average Rank: 17889.94\n",
      "\n",
      "=== Identification Experiment with 55 bins ===\n",
      "Top-1 Success: 0.00%\n",
      "Average Rank: 17948.10\n",
      "\n",
      "=== Identification Experiment with 65 bins ===\n",
      "Top-1 Success: 0.00%\n",
      "Average Rank: 18061.37\n",
      "\n",
      "=== Identification Experiment with 75 bins ===\n",
      "Top-1 Success: 0.00%\n",
      "Average Rank: 18145.35\n",
      "\n",
      "=== Identification Experiment with 85 bins ===\n",
      "Top-1 Success: 0.05%\n",
      "Average Rank: 17854.94\n",
      "\n",
      "=== Identification Experiment with 95 bins ===\n",
      "Top-1 Success: 0.00%\n",
      "Average Rank: 17750.05\n"
     ]
    }
   ],
   "source": [
    "# put it all together\n",
    "n = 6\n",
    "samples, mu, cov = generate_random_samples(n, scale=2.0, size=100000)\n",
    "for n_bins in range(5,100,10):\n",
    "    print(f\"\\n=== Identification Experiment with {n_bins} bins ===\")\n",
    "\n",
    "    discretized_samples = discretize_to_percentiles(samples, n_bins=n_bins)\n",
    "\n",
    "    # get train test split of both continuous and discretized samples\n",
    "    # to do pick a method that makes it clearer these have been split the same way\n",
    "    continuous_train, continuous_test = train_test_split(samples, test_size=0.4, random_state=42)\n",
    "    discretised_train, discretised_test = train_test_split(discretized_samples, test_size=0.4, random_state=42)\n",
    "\n",
    "\n",
    "    identification_experiment(discretised_train, discretised_test, bins=n_bins)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b8051bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Histogram: [  1   0   6  36 145 276 320 165  47   4]\n",
      "Bin edges: [-24.02273693 -19.95944988 -15.89616283 -11.83287578  -7.76958874\n",
      "  -3.70630169   0.35698536   4.42027241   8.48355946  12.54684651\n",
      "  16.61013356]\n"
     ]
    }
   ],
   "source": [
    "# bin data into 10 bins\n",
    "hist, bin_edges = np.histogram(x, bins=10)\n",
    "print(\"Histogram:\", hist)\n",
    "print(\"Bin edges:\", bin_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb25de04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating discretization with 4 bins ===\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# Set reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulation parameters\n",
    "N_total = 1000000       # total population\n",
    "N_train = 800000       # training cohort\n",
    "N_test = N_total - N_train\n",
    "D = 6                # number of variables\n",
    "bins_list = [4, 8, 16, 36, 64, 128]  # discretization levels\n",
    "\n",
    "# Generate correlated multivariate data\n",
    "mu = np.zeros(D)\n",
    "Sigma = 0.5 * np.ones((D, D)) + 0.5 * np.eye(D)  # correlated structure\n",
    "latent = np.random.multivariate_normal(mu, Sigma, size=N_total)\n",
    "\n",
    "# Split train/test\n",
    "latent_train = latent[:N_train]\n",
    "latent_test  = latent[N_train:]\n",
    "\n",
    "def quantile_discretize(arr, bins):\n",
    "    \"\"\"Discretize each column into equal-frequency quantile bins.\"\"\"\n",
    "    arr_disc = np.zeros_like(arr, dtype=int)\n",
    "    for j in range(arr.shape[1]):\n",
    "        ranks = arr[:, j].argsort().argsort()\n",
    "        arr_disc[:, j] = np.ceil((ranks + 1) / len(ranks) * bins).astype(int)\n",
    "        arr_disc[arr_disc[:, j] > bins, j] = bins\n",
    "    return arr_disc\n",
    "\n",
    "def identify_accuracy(train, test, bins):\n",
    "    \"\"\"Estimate identifiability for given discretization.\"\"\"\n",
    "    # Estimate mean and covariance from training data\n",
    "    mu_hat = train.mean(axis=0)\n",
    "    Sigma_hat = np.cov(train, rowvar=False)\n",
    "    inv_Sigma = np.linalg.pinv(Sigma_hat)\n",
    "    \n",
    "    correct = 0\n",
    "    known_var = 0  # assume we only know variable 0 for reidentification\n",
    "\n",
    "    # For each test subject\n",
    "    for i, x in enumerate(test):\n",
    "        # true observed value\n",
    "        x_obs = x[known_var]\n",
    "        # compute conditional distribution of X_known given others\n",
    "        # Using covariance to get predicted mean for known variable given the rest\n",
    "        likelihoods = []\n",
    "        for j, candidate in enumerate(test):\n",
    "            diff = candidate - mu_hat\n",
    "            # Mahalanobis distance as a proxy for \"match likelihood\"\n",
    "            mdist = diff @ inv_Sigma @ diff.T\n",
    "            likelihoods.append(-mdist)\n",
    "        # guess = index of highest \"likelihood\"\n",
    "        guess = np.argmax(likelihoods)\n",
    "        if guess == i:\n",
    "            correct += 1\n",
    "    return correct / len(test)\n",
    "\n",
    "# Run for each discretization level\n",
    "results = []\n",
    "for bins in bins_list:\n",
    "    print(f\"\\n=== Evaluating discretization with {bins} bins ===\")\n",
    "    disc = quantile_discretize(latent, bins)\n",
    "    train_disc = disc[:N_train]\n",
    "    test_disc  = disc[N_train:]\n",
    "    acc = identify_accuracy(train_disc, test_disc, bins)\n",
    "    print(f\"Identifiability (Top-1 Accuracy): {acc*100:.2f}%\")\n",
    "    results.append((bins, acc))\n",
    "\n",
    "df = pd.DataFrame(results, columns=[\"Discretization_bins\", \"Identifiability\"])\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "info-gov-war",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
